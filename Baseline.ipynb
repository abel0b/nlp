{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Générons une prédiction valide pour le challenge\n",
    "\n",
    "Imports nécessaires au notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import multiprocessing\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commencez par définir le nom de votre équipe termes alpha numériques sans espace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEAM_NAME = \"Wasabi\"\n",
    "assert re.match(\"^\\w+$\", TEAM_NAME) is not None, \"Nom d'équipe invalide\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idp</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63610</td>\n",
       "      <td>4pairs silicone Oreillettes de remplacement du...</td>\n",
       "      <td>4pairs silicone Oreillettes de remplacement du...</td>\n",
       "      <td>4.90</td>\n",
       "      <td>TV - VIDEO - SON &gt; CASQUE - MICROPHONE - DICTA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>580661</td>\n",
       "      <td>Dvd Rafaela Legouvello</td>\n",
       "      <td>Une aventure humaine hors du commun...</td>\n",
       "      <td>14.89</td>\n",
       "      <td>DVD - BLU-RAY &gt; DVD &gt; DVD DOCUMENTAIRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90191</td>\n",
       "      <td>Q2671a (H.309Ac) Toner Laser Hp Bleu (Cyan), ...</td>\n",
       "      <td>Q2671A (H.309AC)   TONER LASER HP Bleu (Cyan),...</td>\n",
       "      <td>53.40</td>\n",
       "      <td>INFORMATIQUE &gt; IMPRESSION - SCANNER &gt; TONER - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1297725</td>\n",
       "      <td>Azalaïs ou La vie courtoise</td>\n",
       "      <td>Maryse Rouy</td>\n",
       "      <td>19.05</td>\n",
       "      <td>LIBRAIRIE &gt; LITTERATURE &gt; ROMANS HISTORIQUES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>119129</td>\n",
       "      <td>Hamecon Coup Vmc Crystal 9408 Bronze (50 N°8)</td>\n",
       "      <td>Hameçon coup Crystal 9408 bronzé VMC.   Forme ...</td>\n",
       "      <td>7.59</td>\n",
       "      <td>SPORT &gt; PECHE &gt; HAMEÇON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       idp                                              title  \\\n",
       "0    63610  4pairs silicone Oreillettes de remplacement du...   \n",
       "1   580661                             Dvd Rafaela Legouvello   \n",
       "2    90191   Q2671a (H.309Ac) Toner Laser Hp Bleu (Cyan), ...   \n",
       "3  1297725                        Azalaïs ou La vie courtoise   \n",
       "4   119129      Hamecon Coup Vmc Crystal 9408 Bronze (50 N°8)   \n",
       "\n",
       "                                         description  price  \\\n",
       "0  4pairs silicone Oreillettes de remplacement du...   4.90   \n",
       "1             Une aventure humaine hors du commun...  14.89   \n",
       "2  Q2671A (H.309AC)   TONER LASER HP Bleu (Cyan),...  53.40   \n",
       "3                                        Maryse Rouy  19.05   \n",
       "4  Hameçon coup Crystal 9408 bronzé VMC.   Forme ...   7.59   \n",
       "\n",
       "                                            category  \n",
       "0  TV - VIDEO - SON > CASQUE - MICROPHONE - DICTA...  \n",
       "1             DVD - BLU-RAY > DVD > DVD DOCUMENTAIRE  \n",
       "2  INFORMATIQUE > IMPRESSION - SCANNER > TONER - ...  \n",
       "3       LIBRAIRIE > LITTERATURE > ROMANS HISTORIQUES  \n",
       "4                            SPORT > PECHE > HAMEÇON  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chargement des données d'entrainement\n",
    "train = pd.read_csv(\"train.csv.gz\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idp</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4873686</td>\n",
       "      <td>Endoscope Sans Fil Wifi 1200 P Caméra D'inspec...</td>\n",
       "      <td>Descriptions&amp;nbsp;Résolution: 640 * 480, 1280 ...</td>\n",
       "      <td>16.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6320508</td>\n",
       "      <td>Aider l'enfant dyslexique</td>\n",
       "      <td>Bernard Jumel   3e édition</td>\n",
       "      <td>16.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6351042</td>\n",
       "      <td>Modà ̈Le réduit de véhicule de construction Li...</td>\n",
       "      <td>Modà¨le réduit de véhicule de construction Lie...</td>\n",
       "      <td>24.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3853418</td>\n",
       "      <td>Lampenwelt lampadaire Led à intensité variable...</td>\n",
       "      <td>Ce lampadaire séduit par son design esthétique...</td>\n",
       "      <td>167.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6373582</td>\n",
       "      <td>Caméra vidéo caméscope, gordvec 2,7 Pouces écr...</td>\n",
       "      <td>\"Caractéristiques : 2,7 pouces (16 : 9) LCD, l...</td>\n",
       "      <td>98.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       idp                                              title  \\\n",
       "0  4873686  Endoscope Sans Fil Wifi 1200 P Caméra D'inspec...   \n",
       "1  6320508                          Aider l'enfant dyslexique   \n",
       "2  6351042  Modà ̈Le réduit de véhicule de construction Li...   \n",
       "3  3853418  Lampenwelt lampadaire Led à intensité variable...   \n",
       "4  6373582  Caméra vidéo caméscope, gordvec 2,7 Pouces écr...   \n",
       "\n",
       "                                         description   price  \n",
       "0  Descriptions&nbsp;Résolution: 640 * 480, 1280 ...   16.85  \n",
       "1                         Bernard Jumel   3e édition   16.90  \n",
       "2  Modà¨le réduit de véhicule de construction Lie...   24.97  \n",
       "3  Ce lampadaire séduit par son design esthétique...  167.90  \n",
       "4  \"Caractéristiques : 2,7 pouces (16 : 9) LCD, l...   98.99  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chargement des données à prédire\n",
    "test = pd.read_csv(\"test.csv.gz\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combien y a t'il de catégories produits distinctes dans le dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.category.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ca fait beaucoup! Jusqu'ici vous n'aviez que 2 classes à prédire.\n",
    "\n",
    "## Baseline : \n",
    "On va commencer par la prédiction la plus simple possible, une prédiction constante.\n",
    "\n",
    "On prédit que toutes les catégories sont égales à la 1ère catégorie du dataset de train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On va toujours prédire: 'TV - VIDEO - SON > CASQUE - MICROPHONE - DICTAPHONE > CASQUE - ECOUTEURS'\n"
     ]
    }
   ],
   "source": [
    "category = train.iloc[0].category\n",
    "print(f\"On va toujours prédire: '{category}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn a même un modèle tout fait pour se genre de cas d'usage : DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DummyClassifier(strategy='constant', constant=category)\n",
    "model.fit(train[\"title\"], train[\"category\"])\n",
    "y_pred = model.predict(test[\"title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "et voici notre prédiction dans le bon format : 2 colonnes, l'identifiant produit suivi de sa catégorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idp</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4873686</td>\n",
       "      <td>TV - VIDEO - SON &gt; CASQUE - MICROPHONE - DICTA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6320508</td>\n",
       "      <td>TV - VIDEO - SON &gt; CASQUE - MICROPHONE - DICTA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6351042</td>\n",
       "      <td>TV - VIDEO - SON &gt; CASQUE - MICROPHONE - DICTA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3853418</td>\n",
       "      <td>TV - VIDEO - SON &gt; CASQUE - MICROPHONE - DICTA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6373582</td>\n",
       "      <td>TV - VIDEO - SON &gt; CASQUE - MICROPHONE - DICTA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       idp                                           category\n",
       "0  4873686  TV - VIDEO - SON > CASQUE - MICROPHONE - DICTA...\n",
       "1  6320508  TV - VIDEO - SON > CASQUE - MICROPHONE - DICTA...\n",
       "2  6351042  TV - VIDEO - SON > CASQUE - MICROPHONE - DICTA...\n",
       "3  3853418  TV - VIDEO - SON > CASQUE - MICROPHONE - DICTA...\n",
       "4  6373582  TV - VIDEO - SON > CASQUE - MICROPHONE - DICTA..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = pd.DataFrame({\"idp\": test[\"idp\"], \"category\" : y_pred})\n",
    "prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La fonction de publication de vos prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions est un dataframe qui contient au moins les colonnes \"idp\" et \"category\"\n",
    "# Ca écrit simplement le fichier dans le répertoire d'évaluation\n",
    "def publish_results(predictions):\n",
    "    now = int(time.time())\n",
    "    assert re.match(\"^\\w+$\", TEAM_NAME) is not None\n",
    "    filename = f\"/home/cisd-jacq/projet/evaluation/prediction-{TEAM_NAME}-{now}.csv.gz\"\n",
    "    predictions[[\"idp\", \"category\"]].to_csv(filename, index=False, compression=\"gzip\")\n",
    "    return filename.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois que vous avez spécifié un nom d'équipe vous pouvez soumettre votre 1ère prédiction en exécutant la cellule suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publish_results(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La fonction d'évaluation de l'erreur\n",
    "\n",
    "Vous pouvez utiliser cette fonction pour estimer sur un sous ensemble du dataset train quelle est la précision de votre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plus ce score est grand moins on est content\n",
    "def error(real_category, predicted_category):\n",
    "    # On a trouvé la bonne catégorie\n",
    "    if real_category == predicted_category:\n",
    "        return 0\n",
    "    \n",
    "    # On extrait les sous catégories\n",
    "    real_categories = real_category.split(\" > \")\n",
    "    pred_categories = predicted_category.split(\" > \")\n",
    "    # On flag les catégories adultes\n",
    "    adult_categories = ['ADULTE - EROTIQUE', 'VIN - ALCOOL - LIQUIDES']\n",
    "    real_is_adult = real_categories[0] in adult_categories\n",
    "    pred_is_adult = pred_categories[0] in adult_categories\n",
    "    \n",
    "    # Attention non symmétrie de l'erreur !\n",
    "    if real_is_adult and not pred_is_adult:\n",
    "        error = 10_000\n",
    "        return error\n",
    "    \n",
    "    # On identifie à quel niveau on s'est trompé\n",
    "    for real, pred, error in zip(real_categories, pred_categories, [100, 10, 1]):\n",
    "        if real != pred:\n",
    "            return error\n",
    "    raise ValueError(\"Catégories différentes, mais aucune différence trouvée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Une prédiction moins dummy\n",
    "\n",
    "Le dataset est bien plus volumineux que les datasets utilisés en TP jusqu'à aujourd'hui.\n",
    "\n",
    "Si vous vous lancez tête baissée à générer une prédiction vous allez attendre longtemps d'obtenir vos résultats. Il y a 2 raisons à cela : \n",
    "- Le nombre de lignes de l'ensemble train est très important\n",
    "- Le nombre de classes à prédire est aussi très grand. Certains classifier multi-classe génèrent des classifier binaires de type One vs Rest. Si vous avez 600 classes vous allez apprendre 600 classifiers différents\n",
    "\n",
    "Simplifions donc ces 2 problèmes.\n",
    "\n",
    "## 1. Travaillons sur 10% du dataset de train\n",
    "\n",
    "Commencons par itérer rapidement sur le jeu de données. Une fois qu'on aurra un modèle qui nous convient on pourra travailler sur plus de volumétrie.\n",
    "\n",
    "Générez train_subset, un sample de train faisant 10% de sa taille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset_size = int(0.1*train.shape[0])\n",
    "chosen_idx = np.random.choice(train_subset_size, replace=False, size=train_subset_size)\n",
    "train_subset = train.iloc[chosen_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Limitons les classes\n",
    "\n",
    "600 classes c'est beaucoup trop. \n",
    "\n",
    "On a vu que les catégories contiennent une hiérarchie \"catégorie 1 > catégorie 2 > catégorie 3\", commencons par travailler uniquement sur les catégories 1.\n",
    "\n",
    "Générez la colonne \"category_1\" dans train_subset à partir de la colonne \"category\" qui contient uniquement la catégorie 1 du produit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_category(full_category):\n",
    "    return full_category.split(\">\")[0].strip()\n",
    "if 'category_1' in train_subset.columns:\n",
    "    train_subset = train_subset.drop(columns=[\"category_1\"])\n",
    "train_subset.insert(train_subset.shape[1], \"category_1\", list(map(first_category, train_subset[\"category\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_freq = {}\n",
    "for el in train_subset.iterrows():\n",
    "    cat1 = el[1][\"category_1\"]\n",
    "    if cat1 not in categories_freq:\n",
    "        categories_freq[cat1] = {\"max_freq\" : 0, \"max_cat\" : el[1][\"category\"]}\n",
    "    \n",
    "    if el[1][\"category\"] not in categories_freq[cat1]:\n",
    "        categories_freq[cat1][el[1][\"category\"]] = 1\n",
    "    else :\n",
    "        categories_freq[cat1][el[1][\"category\"]] += 1\n",
    "        if categories_freq[cat1][\"max_freq\"] < categories_freq[cat1][el[1][\"category\"]]:\n",
    "            categories_freq[cat1][\"max_freq\"] += 1 \n",
    "            categories_freq[cat1][\"max_cat\"] = el[1][\"category\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combien de \"category_1\" distinctes vous avez?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbcat1 = len(np.unique(train_subset[\"category_1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est bien plus raisonnable pour commencer.\n",
    "\n",
    "## 3. Oh oh il y a pleins de features.\n",
    "\n",
    "Il y a pleins de features différentes.\n",
    "\n",
    "- 2 champs de texte title et description \n",
    "- 1 champ float : price.\n",
    "\n",
    "On va se contenter de travailler uniquement avec \"title\" pour commencer.\n",
    "\n",
    "Générez les features : X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/cisd-\n",
      "[nltk_data]     calluau/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "def process_text(line):\n",
    "    tokens = nltk.word_tokenize(line)\n",
    "    new_tokens = []\n",
    "    pattern = '\\w'\n",
    "    for token in tokens:\n",
    "        if len(token) > 1 and token not in stop_words and re.match(pattern, token):\n",
    "            new_tokens.append(token.lower())\n",
    "    return new_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'title_tokens' in train_subset.columns:\n",
    "    train_subset = train_subset.drop(columns=[\"title_tokens\"])\n",
    "    \n",
    "train_subset.insert(train_subset.shape[1], \"title_tokens\", np.array([process_text(x) for x in train_subset[\"title\"].to_numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_id_to_words(data, field):\n",
    "    words_to_id = {}\n",
    "    nextid = 0\n",
    "    for row in data[field]:\n",
    "        for word in row:\n",
    "            if word not in words_to_id:\n",
    "                words_to_id[word] = nextid\n",
    "                nextid += 1\n",
    "    return words_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'title_tokens' in test:\n",
    "    test = test.drop(columns=[\"title_tokens\"])\n",
    "    \n",
    "nproc = 20\n",
    "manager = multiprocessing.Manager()\n",
    "title_tokens = manager.list()\n",
    "titles = test[\"title\"].to_numpy()\n",
    "\n",
    "jobs = []\n",
    "def vprocess_text(start, end):\n",
    "    global title_tokens\n",
    "    global titles\n",
    "    for i in range(start, end):\n",
    "        title_tokens.append(process_text(titles[i]))\n",
    "\n",
    "for pid in range(nproc):\n",
    "    job = multiprocessing.Process(target=vprocess_text, args=[int(pid*len(titles)/nproc), len(titles) if pid+1==nproc else int((pid+1)*len(titles)/nproc)])\n",
    "    job.start()\n",
    "    jobs.append(job)\n",
    "    \n",
    "for job in jobs:\n",
    "    job.join()\n",
    "\n",
    "del titles\n",
    "\n",
    "test.insert(test.shape[1], \"title_tokens\", np.array(title_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'title_tokens' in test:\n",
    "#     test = test.drop(columns=[\"title_tokens\"])\n",
    "# test.insert(test.shape[1], \"title_tokens\", np.array([process_text(x) for x in test[\"title\"].to_numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_id = assign_id_to_words(train_subset, \"title_tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_to_id = {}\n",
    "nextid = 0\n",
    "for cat in train_subset[\"category_1\"]:\n",
    "    if cat not in categories_to_id:\n",
    "         categories_to_id[cat] = nextid\n",
    "         nextid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_category = {v: k for k, v in categories_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_to_vector(bag_of_words, words_to_id):\n",
    "    features = np.zeros(len(words_to_id))\n",
    "    for word in bag_of_words:\n",
    "        if word in words_to_id:\n",
    "            features[words_to_id[word]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = multiprocessing.Manager()\n",
    "\n",
    "X_train = manager.list()\n",
    "y_train = manager.list()\n",
    "\n",
    "def build_X_train():\n",
    "    global X_train\n",
    "    for row in train_subset.iterrows():\n",
    "        X_train.append(bag_of_words_to_vector(row[1][\"title_tokens\"], words_to_id))\n",
    "    X_train = np.array(X_train)\n",
    "    \n",
    "def build_y_train():\n",
    "    global y_train\n",
    "    for row in train_subset.iterrows():\n",
    "        y_train.append(categories_to_id[row[1][\"category_1\"]])\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "p_X_train = multiprocessing.Process(target=build_X_train)\n",
    "p_y_train = multiprocessing.Process(target=build_y_train)\n",
    "p_X_train.start()\n",
    "p_y_train.start()\n",
    "p_X_train.join()\n",
    "p_y_train.join()\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "features_dim = len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2, ..., 10,  9,  9])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculez un modèle avec ces simplifications\n",
    "\n",
    "Cf TP1 et TP2 : Entrainez une régression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cisd-calluau/nlp/python/lib64/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sklearn.linear_model.LogisticRegression()\n",
    "\n",
    "del train\n",
    "del train_subset\n",
    "del stop_words\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"logistic.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculez votre prédiction\n",
    "\n",
    "Appliquez votre modèle sur les données test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-94220f4d799c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "confusion_matrix = sklearn.metrics.confusion_matrix(y_train, model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.model_selection.cross_validate(model, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Soumettez votre prédiction.\n",
    "\n",
    "*Pas si vite* : Vous ne prédisez que la catégorie 1. Le script d'évaluation attend une catégorie complète...\n",
    "\n",
    "C'est simple, pour chaque catégorie 1, choisissez la catégorie de votre choix qui commence par cette \"categorie 1\".\n",
    "\n",
    "Modifiez votre prédiction, y_pred, en conséquence.\n",
    "\n",
    "Soumettez votre prédiction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "test_titles = test[\"title_tokens\"].to_numpy()\n",
    "\n",
    "batch_size = 80000\n",
    "i = 0\n",
    "while i < test.shape[0]:\n",
    "    batch = np.array([bag_of_words_to_vector(title_tokens, words_to_id) for title_tokens in test_titles[i:min(i+batch_size, test.shape[0])]])\n",
    "    batch_pred = model.predict(batch)\n",
    "    y_pred = y_pred + list(map(lambda cat_id: categories_freq[id_to_category[cat_id]][\"max_cat\"], batch_pred))\n",
    "    i += batch_size\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame({\"idp\": test[\"idp\"], \"category\" : y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prediction-Wasabi-1578867760.csv.gz'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publish_results(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez regardez vos scores en exécutant le notebook [Leaderboard.ipynb](Leaderboard.ipynb). Les données sont mises à jour toutes les 30min.\n",
    "\n",
    "# Sauvegardez votre modèle\n",
    "\n",
    "Pour ne pas devoir recommencer à chaque fois tout ce dur labeur, et cette longue attente, vous pouvez sauvegarder votre modèle et votre vectorizer.\n",
    "\n",
    "La prochaine fois vous n'aurez qu'à les recharger pour faire directement vos prédictions (c'est ce qui est attendu pour la soutenance, sinon le timing sera trop serré) \n",
    "\n",
    "La documentation : https://scikit-learn.org/stable/modules/model_persistence.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maintenant à vous de jouer\n",
    "\n",
    "Vous pouvez commencer par travailler sur plus de volumétrie que 5% du dataset. Mais maintenant le challenge commence.\n",
    "\n",
    "Si vous ne savez pas par où commencer suivez le déroulement des 2 premiers TPs, en prenant garde à la volumétrie. Ils sont disponibles dans le répertoire ~cisd-jacq/TP/\n",
    "\n",
    "Contrairement aux TPs vous n'avez pas d'information sur les données de test. (Mis à part le score calculé toute les 30min).\n",
    "\n",
    "Pour évaluer votre modèle et l'améliorer vous pouvez utiliser les données de train pour crééer un ensemble d'entrainement et un ensemble de validation. \n",
    "\n",
    "Vous pourrez alors évaluer plus rapidement vos modèles et identifier quelles sont les catégories sur lesquelles vous devez vous améliorer.\n",
    "\n",
    "Il n'y a pas que la Régression Logistique dans la vie, essayez d'autres modèles. Je vous ai fait travailler avec sklearn, mais il existe aussi d'autres librairies.\n",
    "\n",
    "Pour paralléliser vos calculs :\n",
    "- multiprocessing : https://docs.python.org/3/library/multiprocessing.html\n",
    "- dask : https://dask.org/ + https://distributed.dask.org/en/latest/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_generator():\n",
    "    for row in train.iterrows():\n",
    "        for word in row[1][\"title\"]:\n",
    "            yield word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_filename = \"train.vocab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.SubwordTextEncoder.build_from_corpus(corpus_generator(), target_vocab_size=2**15)\n",
    "encoder.save_to_file(vocab_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.SubwordTextEncoder.load_from_file(vocab_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 146)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = []\n",
    "dim = 0\n",
    "\n",
    "for row in train_subset.iterrows():\n",
    "    encoded = encoder.encode(row[1][\"title\"])\n",
    "    dim = max(dim, len(encoded))\n",
    "    X_train.append(encoded)\n",
    "\n",
    "X_train = [x+[0.0 for i in range(dim-len(x)) ] for x in X_train]\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 146)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cisd-calluau/nlp/python/lib64/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sklearn.linear_model.LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 146)\n",
      "(80000, 146)\n",
      "(80000, 146)\n",
      "(80000, 146)\n",
      "(80000, 146)\n",
      "(80000, 146)\n",
      "(80000, 146)\n",
      "(80000, 146)\n",
      "(80000, 146)\n",
      "(80000, 146)\n",
      "(80000, 146)\n",
      "(80000, 146)\n",
      "(40000, 146)\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "test_titles = test[\"title\"].to_numpy()\n",
    "\n",
    "batch_size = 80000\n",
    "i = 0\n",
    "while i < test.shape[0]:\n",
    "    batch = []\n",
    "    for title in test_titles[i:min(i+batch_size, test.shape[0])]:\n",
    "        vec = encoder.encode(title)\n",
    "        if len(vec) < dim:\n",
    "            vec = vec + [0.0 for i in range(dim-len(vec))]\n",
    "        elif len(vec) > dim:\n",
    "            vec = vec[0:dim]\n",
    "        if len(vec) != 146:\n",
    "            print(len(vec))\n",
    "        batch.append(vec)\n",
    "    batch = np.array(batch)\n",
    "    print(batch.shape)\n",
    "    batch_pred = model.predict(batch)\n",
    "    y_pred = y_pred + list(map(lambda cat_id: categories_freq[id_to_category[cat_id]][\"max_cat\"], batch_pred))\n",
    "    i += batch_size\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prediction-Wasabi-1578908336.csv.gz'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = pd.DataFrame({\"idp\": test[\"idp\"], \"category\" : y_pred})\n",
    "publish_results(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 64)          28416     \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 30)                1950      \n",
      "=================================================================\n",
      "Total params: 104,670\n",
      "Trainable params: 104,670\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(embedding_dim=64):\n",
    "    model = tensorflow.keras.Sequential([\n",
    "        # Add an Embedding layer expecting input vocab of size 5000, and output embedding dimension of size 64 we set at the top\n",
    "        tensorflow.keras.layers.Embedding(encoder.vocab_size, embedding_dim),\n",
    "        tensorflow.keras.layers.Bidirectional(tensorflow.keras.layers.LSTM(embedding_dim)),\n",
    "        # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "        # use ReLU in place of tanh function since they are very good alternatives of each other.\n",
    "        tensorflow.keras.layers.Dense(embedding_dim, activation='relu'),\n",
    "        # Add a Dense layer with 6 units and softmax activation.\n",
    "        # When we have multiple outputs, softmax convert outputs layers into a probability distribution.\n",
    "        tensorflow.keras.layers.Dense(len(np.unique(train_subset[\"category_1\"])), activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30000 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history = model.fit(X_train, y_train, epochs=num_epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
