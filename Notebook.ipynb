{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Générons une prédiction valide pour le challenge\n",
    "\n",
    "Imports nécessaires au notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from multiprocessing import Manager, Process\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nom de l'équipe : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEAM_NAME = \"Wasabi\"\n",
    "assert re.match(\"^\\w+$\", TEAM_NAME) is not None, \"Nom d'équipe invalide\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données d'entrainement\n",
    "train = pd.read_csv(\"train.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données à prédire\n",
    "test = pd.read_csv(\"soutenance.csv.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combien y a t'il de catégories produits distinctes dans le dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.category.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La fonction de publication de vos prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions est un dataframe qui contient au moins les colonnes \"idp\" et \"category\"\n",
    "# Ca écrit simplement le fichier dans le répertoire d'évaluation\n",
    "def publish_results(predictions):\n",
    "    now = int(time.time())\n",
    "    assert re.match(\"^\\w+$\", TEAM_NAME) is not None\n",
    "    filename = f\"/home/cisd-jacq/projet/evaluation/prediction-{TEAM_NAME}-{now}.csv.gz\"\n",
    "    predictions[[\"idp\", \"category\"]].to_csv(filename, index=False, compression=\"gzip\")\n",
    "    return filename.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La fonction d'évaluation de l'erreur\n",
    "\n",
    "Vous pouvez utiliser cette fonction pour estimer sur un sous ensemble du dataset train quelle est la précision de votre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plus ce score est grand moins on est content\n",
    "def error(real_category, predicted_category):\n",
    "    # On a trouvé la bonne catégorie\n",
    "    if real_category == predicted_category:\n",
    "        return 0\n",
    "    \n",
    "    # On extrait les sous catégories\n",
    "    real_categories = real_category.split(\" > \")\n",
    "    pred_categories = predicted_category.split(\" > \")\n",
    "    # On flag les catégories adultes\n",
    "    adult_categories = ['ADULTE - EROTIQUE', 'VIN - ALCOOL - LIQUIDES']\n",
    "    real_is_adult = real_categories[0] in adult_categories\n",
    "    pred_is_adult = pred_categories[0] in adult_categories\n",
    "    \n",
    "    # Attention non symmétrie de l'erreur !\n",
    "    if real_is_adult and not pred_is_adult:\n",
    "        error = 10_000\n",
    "        return error\n",
    "    \n",
    "    # On identifie à quel niveau on s'est trompé\n",
    "    for real, pred, error in zip(real_categories, pred_categories, [100, 10, 1]):\n",
    "        if real != pred:\n",
    "            return error\n",
    "    raise ValueError(\"Catégories différentes, mais aucune différence trouvée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Une prédiction moins dummy\n",
    "\n",
    "\n",
    "## 1. Travaillons sur 10% du dataset de train\n",
    "\n",
    "Commencons par itérer rapidement sur le jeu de données. Une fois qu'on aurra un modèle qui nous convient on pourra travailler sur plus de volumétrie.\n",
    "\n",
    "Générez train_subset, un sample de train faisant 10% de sa taille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = train.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Limitons les classes\n",
    "\n",
    "600 classes c'est beaucoup trop. \n",
    "\n",
    "On a vu que les catégories contiennent une hiérarchie \"catégorie 1 > catégorie 2 > catégorie 3\", commencons par travailler uniquement sur les catégories 1.\n",
    "\n",
    "Générez la colonne \"category_1\" dans train_subset à partir de la colonne \"category\" qui contient uniquement la catégorie 1 du produit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset[\"category_1\"] = train_subset[\"category\"].str.split(\">\", n=1, expand=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_freq = {}\n",
    "for el in train_subset.iterrows():\n",
    "    cat1 = el[1][\"category_1\"]\n",
    "    if cat1 not in categories_freq:\n",
    "        categories_freq[cat1] = {\"max_freq\" : 0, \"max_cat\" : el[1][\"category\"]}\n",
    "    \n",
    "    if el[1][\"category\"] not in categories_freq[cat1]:\n",
    "        categories_freq[cat1][el[1][\"category\"]] = 1\n",
    "    else :\n",
    "        categories_freq[cat1][el[1][\"category\"]] += 1\n",
    "        if categories_freq[cat1][\"max_freq\"] < categories_freq[cat1][el[1][\"category\"]]:\n",
    "            categories_freq[cat1][\"max_freq\"] += 1 \n",
    "            categories_freq[cat1][\"max_cat\"] = el[1][\"category\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combien de \"category_1\" distinctes vous avez?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbcat1 = len(np.unique(train_subset[\"category_1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est bien plus raisonnable pour commencer.\n",
    "\n",
    "## 3. Oh oh il y a pleins de features.\n",
    "\n",
    "Il y a pleins de features différentes.\n",
    "\n",
    "- 2 champs de texte title et description \n",
    "- 1 champ float : price.\n",
    "\n",
    "On va se contenter de travailler uniquement avec \"title\" pour commencer.\n",
    "\n",
    "Générez les features : X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/cisd-\n",
      "[nltk_data]     cazalet/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "def tokenize_text(line):\n",
    "    tokens = nltk.word_tokenize(line)\n",
    "    new_tokens = []\n",
    "    pattern = '\\w'\n",
    "    for token in tokens:\n",
    "        if len(token) > 1 and token not in stop_words and re.match(pattern, token):\n",
    "            new_tokens.append(token.lower())\n",
    "    return new_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'title_tokens' in train_subset.columns:\n",
    "    train_subset = train_subset.drop(columns=[\"title_tokens\"])\n",
    "    \n",
    "train_subset.insert(train_subset.shape[1], \"title_tokens\", np.array([tokenize_text(x) for x in train_subset[\"title\"].to_numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_id_to_words(data, field):\n",
    "    words_to_id = {}\n",
    "    nextid = 0\n",
    "    for row in data[field]:\n",
    "        for word in row:\n",
    "            if word not in words_to_id:\n",
    "                words_to_id[word] = nextid\n",
    "                nextid += 1\n",
    "    return words_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On tokenise la colonne 'title' de l'ensemble test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'title_tokens' in test:\n",
    "    test = test.drop(columns=[\"title_tokens\"])\n",
    "\n",
    "nproc = 20\n",
    "manager = Manager()\n",
    "title_tokens = manager.list()\n",
    "titles = test[\"title\"].to_numpy()\n",
    "\n",
    "jobs = []\n",
    "def vprocess_text(start, end):\n",
    "    global title_tokens\n",
    "    global titles\n",
    "    for i in range(start, end):\n",
    "        title_tokens.append(tokenize_text(titles[i]))\n",
    "\n",
    "for pid in range(nproc):\n",
    "    job = Process(target=vprocess_text, args=[int(pid*len(titles)/nproc), len(titles) if pid+1==nproc else int((pid+1)*len(titles)/nproc)])\n",
    "    job.start()\n",
    "    jobs.append(job)\n",
    "\n",
    "for job in jobs:\n",
    "    job.join()\n",
    "\n",
    "del titles\n",
    "test.insert(test.shape[1], \"title_tokens\", np.array(title_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'title_tokens' in test:\n",
    "#     test = test.drop(columns=[\"title_tokens\"])\n",
    "# test.insert(test.shape[1], \"title_tokens\", np.array([tokenize_text(x) for x in test[\"title\"].to_numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_id = assign_id_to_words(train_subset, \"title_tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_to_id = {}\n",
    "nextid = 0\n",
    "for cat in train_subset[\"category_1\"]:\n",
    "    if cat not in categories_to_id:\n",
    "         categories_to_id[cat] = nextid\n",
    "         nextid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_category = {v: k for k, v in categories_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_to_vector(bag_of_words, words_to_id):\n",
    "    features = np.zeros(len(words_to_id))\n",
    "    for word in bag_of_words:\n",
    "        if word in words_to_id:\n",
    "            features[words_to_id[word]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À partir de l'ensemble d'entrainement, on créer le X_train et le Y_train pour entrainer notre modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "\n",
    "X_train = manager.list()\n",
    "y_train = manager.list()\n",
    "\n",
    "def build_X_train():\n",
    "    global X_train\n",
    "    for row in train_subset.iterrows():\n",
    "        X_train.append(bag_of_words_to_vector(row[1][\"title_tokens\"], words_to_id))\n",
    "    X_train = np.array(X_train)\n",
    "    \n",
    "def build_y_train():\n",
    "    global y_train\n",
    "    for row in train_subset.iterrows():\n",
    "        y_train.append(categories_to_id[row[1][\"category_1\"]])\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "p_X_train = Process(target=build_X_train)\n",
    "p_y_train = Process(target=build_y_train)\n",
    "p_X_train.start()\n",
    "p_y_train.start()\n",
    "p_X_train.join()\n",
    "p_y_train.join()\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "features_dim = len(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculez un modèle avec ces simplifications\n",
    "\n",
    "On entraine une régression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sauvegardez votre modèle\n",
    "\n",
    "Pour ne pas devoir recommencer à chaque fois tout ce dur labeur, et cette longue attente, vous pouvez sauvegarder votre modèle et votre vectorizer.\n",
    "\n",
    "La prochaine fois vous n'aurez qu'à les recharger pour faire directement vos prédictions (c'est ce qui est attendu pour la soutenance, sinon le timing sera trop serré) \n",
    "\n",
    "La documentation : https://scikit-learn.org/stable/modules/model_persistence.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"logistic.model\"\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.. Calculez votre prédiction\n",
    "\n",
    "Appliquez votre modèle sur les données test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_matrix = sklearn.metrics.confusion_matrix(y_train, model.predict(X_train))\n",
    "#confusion_matrix\n",
    "#del confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.model_selection.cross_validate(model, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on a pas assez d'espace disponible pour la prédiction : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del X_train\n",
    "#del y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Soumettez votre prédiction.\n",
    "\n",
    "*Pas si vite* : Vous ne prédisez que la catégorie 1. Le script d'évaluation attend une catégorie complète...\n",
    "\n",
    "C'est simple, pour chaque catégorie 1, choisissez la catégorie de votre choix qui commence par cette \"categorie 1\".\n",
    "\n",
    "Modifiez votre prédiction, y_pred, en conséquence.\n",
    "\n",
    "Soumettez votre prédiction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "test_titles = test[\"title_tokens\"].to_numpy()\n",
    "\n",
    "batch_size = 80000\n",
    "i = 0\n",
    "while i < test.shape[0]:\n",
    "    batch = np.array([bag_of_words_to_vector(title_tokens, words_to_id) for title_tokens in test_titles[i:min(i+batch_size, test.shape[0])]])\n",
    "    batch_pred = model.predict(batch)\n",
    "    y_pred = y_pred + list(map(lambda cat_id: categories_freq[id_to_category[cat_id]][\"max_cat\"], batch_pred))\n",
    "    i += batch_size\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame({\"idp\": test[\"idp\"], \"category\" : y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish_results(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez regardez vos scores en exécutant le notebook [Leaderboard.ipynb](Leaderboard.ipynb). Les données sont mises à jour toutes les 30min."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maintenant à vous de jouer\n",
    "\n",
    "Vous pouvez commencer par travailler sur plus de volumétrie que 5% du dataset. Mais maintenant le challenge commence.\n",
    "\n",
    "Si vous ne savez pas par où commencer suivez le déroulement des 2 premiers TPs, en prenant garde à la volumétrie. Ils sont disponibles dans le répertoire ~cisd-jacq/TP/\n",
    "\n",
    "Contrairement aux TPs vous n'avez pas d'information sur les données de test. (Mis à part le score calculé toute les 30min).\n",
    "\n",
    "Pour évaluer votre modèle et l'améliorer vous pouvez utiliser les données de train pour crééer un ensemble d'entrainement et un ensemble de validation. \n",
    "\n",
    "Vous pourrez alors évaluer plus rapidement vos modèles et identifier quelles sont les catégories sur lesquelles vous devez vous améliorer.\n",
    "\n",
    "Il n'y a pas que la Régression Logistique dans la vie, essayez d'autres modèles. Je vous ai fait travailler avec sklearn, mais il existe aussi d'autres librairies.\n",
    "\n",
    "Pour paralléliser vos calculs :\n",
    "- multiprocessing : https://docs.python.org/3/library/multiprocessing.html\n",
    "- dask : https://dask.org/ + https://distributed.dask.org/en/latest/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avec un tokenizer différent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générateur de corpus\n",
    "def corpus_generator():\n",
    "    for row in train.iterrows():\n",
    "        for word in row[1][\"title\"]:\n",
    "            yield word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_filename = \"train.vocab\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer un encodeur de texte à partir d'un corpus de texte composé des titres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder = tfds.features.text.SubwordTextEncoder.build_from_corpus(corpus_generator(), target_vocab_size=2**15)\n",
    "#encoder.save_to_file(vocab_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.SubwordTextEncoder.load_from_file(vocab_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 141)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = []\n",
    "dim = 0\n",
    "\n",
    "for row in train_subset.iterrows():\n",
    "    encoded = encoder.encode(row[1][\"title\"])\n",
    "    dim = max(dim, len(encoded))\n",
    "    X_train.append(encoded)\n",
    "\n",
    "X_train = [x+[0.0 for i in range(dim-len(x)) ] for x in X_train]\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    y_train\n",
    "except NameError:\n",
    "    y_train = []\n",
    "    for row in train_subset.iterrows():\n",
    "        y_train.append(categories_to_id[row[1][\"category_1\"]])\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    y_train = np.array(y_train)\n",
    "else:\n",
    "    print(\"well, it WASN'T defined after all!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sauvegarde le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"logistic_with_other_tokenizer.model\"\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "test_titles = test[\"title\"].to_numpy()\n",
    "\n",
    "batch_size = 80000\n",
    "i = 0\n",
    "while i < test.shape[0]:\n",
    "    batch = []\n",
    "    for title in test_titles[i:min(i+batch_size, test.shape[0])]:\n",
    "        vec = encoder.encode(title)\n",
    "        if len(vec) < dim:\n",
    "            vec = vec + [0.0 for i in range(dim-len(vec))]\n",
    "        elif len(vec) > dim:\n",
    "            vec = vec[0:dim]\n",
    "        if len(vec) != 146:\n",
    "            print(len(vec))\n",
    "        batch.append(vec)\n",
    "    batch = np.array(batch)\n",
    "    print(batch.shape)\n",
    "    batch_pred = model.predict(batch)\n",
    "    y_pred = y_pred + list(map(lambda cat_id: categories_freq[id_to_category[cat_id]][\"max_cat\"], batch_pred))\n",
    "    i += batch_size\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame({\"idp\": test[\"idp\"], \"category\" : y_pred})\n",
    "publish_results(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réseau de neuronne récurrent\n",
    "\n",
    "On a réalisé l'implémentation d'un réseau de neuronne récurrent comme expliqué dans cet article : https://towardsdatascience.com/multi-class-text-classification-with-lstm-using-tensorflow-2-0-d88627c10a35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 142)         63048     \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 284)               323760    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 142)               40470     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                4290      \n",
      "=================================================================\n",
      "Total params: 431,568\n",
      "Trainable params: 431,568\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(embedding_dim=(len(X_train[0])+1)):\n",
    "    model = tensorflow.keras.Sequential([\n",
    "        tensorflow.keras.layers.Embedding(encoder.vocab_size, embedding_dim),\n",
    "        tensorflow.keras.layers.Bidirectional(tensorflow.keras.layers.LSTM(embedding_dim)),\n",
    "        tensorflow.keras.layers.Dense(embedding_dim, activation='relu'),\n",
    "        tensorflow.keras.layers.Dense(len(np.unique(train_subset[\"category_1\"])), activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30000 samples\n",
      "Epoch 1/17\n",
      "30000/30000 - 129s - loss: 2.2128 - accuracy: 0.3623\n",
      "Epoch 2/17\n",
      "30000/30000 - 129s - loss: 1.8964 - accuracy: 0.4512\n",
      "Epoch 3/17\n",
      "30000/30000 - 129s - loss: 1.6285 - accuracy: 0.5291\n",
      "Epoch 4/17\n",
      "30000/30000 - 129s - loss: 1.4103 - accuracy: 0.5951\n",
      "Epoch 5/17\n",
      "30000/30000 - 129s - loss: 1.2119 - accuracy: 0.6523\n",
      "Epoch 6/17\n",
      "30000/30000 - 129s - loss: 1.0588 - accuracy: 0.6943\n",
      "Epoch 7/17\n",
      "30000/30000 - 128s - loss: 0.9345 - accuracy: 0.7289\n",
      "Epoch 8/17\n",
      "30000/30000 - 128s - loss: 0.8116 - accuracy: 0.7637\n",
      "Epoch 9/17\n",
      "30000/30000 - 127s - loss: 0.7207 - accuracy: 0.7894\n",
      "Epoch 10/17\n",
      "30000/30000 - 126s - loss: 0.6293 - accuracy: 0.8148\n",
      "Epoch 11/17\n",
      "30000/30000 - 126s - loss: 0.5411 - accuracy: 0.8396\n",
      "Epoch 12/17\n",
      "30000/30000 - 126s - loss: 0.4746 - accuracy: 0.8569\n",
      "Epoch 13/17\n",
      "30000/30000 - 126s - loss: 0.4101 - accuracy: 0.8787\n",
      "Epoch 14/17\n",
      "30000/30000 - 125s - loss: 0.3699 - accuracy: 0.8879\n",
      "Epoch 15/17\n",
      "30000/30000 - 126s - loss: 0.2994 - accuracy: 0.9078\n",
      "Epoch 16/17\n",
      "30000/30000 - 126s - loss: 0.2764 - accuracy: 0.9129\n",
      "Epoch 17/17\n",
      "30000/30000 - 125s - loss: 0.2318 - accuracy: 0.9300\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 17\n",
    "history = model.fit(X_train, y_train, epochs=num_epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sauvegarde le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"rnn.model\"\n",
    "model.save('rnn.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tensorflow.keras.models.load_model('rnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 30)\n",
      "[[7.0908445e-18 6.6020569e-08 3.6599912e-09 5.2164545e-08 6.3250676e-09\n",
      "  2.0278440e-11 3.6214634e-10 4.8039500e-10 2.1252086e-13 1.0973860e-07\n",
      "  1.0908095e-05 5.1187732e-07 1.3766445e-05 8.3147403e-05 1.2835205e-10\n",
      "  1.9285442e-16 3.1951049e-04 5.8961655e-06 2.4381713e-10 4.7433235e-09\n",
      "  8.7174544e-07 1.5900586e-11 8.8108670e-12 5.6281412e-04 9.9885714e-01\n",
      "  6.1119718e-09 1.4506807e-04 1.3922382e-11 1.0400946e-07 3.8983330e-11]]\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "vec = encoder.encode(test[\"title\"].to_numpy()[0])\n",
    "prediction = model.predict([vec])\n",
    "print(prediction.shape)\n",
    "print(prediction)\n",
    "print(np.argmax(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "test_titles = test[\"title\"].to_numpy()\n",
    "\n",
    "batch_size = 80000\n",
    "i = 0\n",
    "while i < test.shape[0]:\n",
    "    batch = []\n",
    "    for title in test_titles[i:min(i+batch_size, test.shape[0])]:\n",
    "        vec = encoder.encode(title)\n",
    "        if len(vec) < dim:\n",
    "            vec = vec + [0.0 for i in range(dim-len(vec))]\n",
    "        elif len(vec) > dim:\n",
    "            vec = vec[0:dim]\n",
    "        batch.append(vec)\n",
    "    batch = np.array(batch)\n",
    "    batch_pred = model.predict(batch)\n",
    "    y_pred = y_pred + list(map(lambda pred: categories_freq[id_to_category[np.argmax(pred\n",
    "                                                                                    )]][\"max_cat\"], batch_pred))\n",
    "    i += batch_size\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame({\"idp\": test[\"idp\"], \"category\" : y_pred})\n",
    "publish_results(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idp</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>title_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>510714</td>\n",
       "      <td>Beige et marron Portefeuille femme Iqzco</td>\n",
       "      <td>Matériau extérieur: matière synthétique, intér...</td>\n",
       "      <td>45.99</td>\n",
       "      <td>[beige, marron, portefeuille, femme, iqzco]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2235312</td>\n",
       "      <td>Hama Étui Eva Pour Disque Dur Externe 2,5\" ...</td>\n",
       "      <td>HAMA   ÉTUI EVA POUR DISQUE DUR EXTERNE 2,5\"  ...</td>\n",
       "      <td>14.57</td>\n",
       "      <td>[hama, étui, eva, pour, disque, dur, externe, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5235844</td>\n",
       "      <td>Bandeau Cheveux Femme Vintage, Bandeau Imprimé...</td>\n",
       "      <td>Caractéristiques:100% neuf et haute qualitéLe ...</td>\n",
       "      <td>8.40</td>\n",
       "      <td>[bandeau, cheveux, femme, vintage, bandeau, im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6336566</td>\n",
       "      <td>3pcs plumes de duvet de polyester vers le bas ...</td>\n",
       "      <td>Tissu polyester: doux et confortable, respiran...</td>\n",
       "      <td>24.93</td>\n",
       "      <td>[3pcs, plumes, duvet, polyester, vers, bas, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5214878</td>\n",
       "      <td>Débardeur rouge décoré de fleurs</td>\n",
       "      <td>Débardeur côtelé et moulant.Modèle avec bretel...</td>\n",
       "      <td>14.99</td>\n",
       "      <td>[débardeur, rouge, décoré, fleurs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>1049028</td>\n",
       "      <td>Autocollant de voiture volant Hawk auto Truck ...</td>\n",
       "      <td>Car Decal volant faucon Auto Camion capot côté...</td>\n",
       "      <td>0.93</td>\n",
       "      <td>[applique, mural, luminaire, contemporain, 6w,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>3236083</td>\n",
       "      <td>Canne A Peche 138H 2 0 Prov Bend Baitholder Sn...</td>\n",
       "      <td>Canne A Peche 138H 2/0 Prov Bend Baitholder Sn...</td>\n",
       "      <td>42.99</td>\n",
       "      <td>[harceleurs, l'école, bureau]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>837827</td>\n",
       "      <td>Aléa</td>\n",
       "      <td>Jan Kjrstad   Du monde entier</td>\n",
       "      <td>23.20</td>\n",
       "      <td>[jarretière, optique, duplex, 2.0, mm, multi, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>2150106</td>\n",
       "      <td>Pour Sony Xperia Z5 : Oreillette Bluetooth Ori...</td>\n",
       "      <td>Oreillette sans fil bluetooth v4.1 ultra légèr...</td>\n",
       "      <td>26.99</td>\n",
       "      <td>[maison, poupée, bois, diy, modèle, miniature,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>3397438</td>\n",
       "      <td>Femmes Longline Shurg X8x4r Taille 40</td>\n",
       "      <td>Pattern : slef design   Tissu: polyester   Len...</td>\n",
       "      <td>58.99</td>\n",
       "      <td>[housse, etui, coque, alcatel, one, touch, ido...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           idp                                              title  \\\n",
       "0       510714           Beige et marron Portefeuille femme Iqzco   \n",
       "1      2235312     Hama Étui Eva Pour Disque Dur Externe 2,5\" ...   \n",
       "2      5235844  Bandeau Cheveux Femme Vintage, Bandeau Imprimé...   \n",
       "3      6336566  3pcs plumes de duvet de polyester vers le bas ...   \n",
       "4      5214878                   Débardeur rouge décoré de fleurs   \n",
       "...        ...                                                ...   \n",
       "99995  1049028  Autocollant de voiture volant Hawk auto Truck ...   \n",
       "99996  3236083  Canne A Peche 138H 2 0 Prov Bend Baitholder Sn...   \n",
       "99997   837827                                               Aléa   \n",
       "99998  2150106  Pour Sony Xperia Z5 : Oreillette Bluetooth Ori...   \n",
       "99999  3397438              Femmes Longline Shurg X8x4r Taille 40   \n",
       "\n",
       "                                             description  price  \\\n",
       "0      Matériau extérieur: matière synthétique, intér...  45.99   \n",
       "1      HAMA   ÉTUI EVA POUR DISQUE DUR EXTERNE 2,5\"  ...  14.57   \n",
       "2      Caractéristiques:100% neuf et haute qualitéLe ...   8.40   \n",
       "3      Tissu polyester: doux et confortable, respiran...  24.93   \n",
       "4      Débardeur côtelé et moulant.Modèle avec bretel...  14.99   \n",
       "...                                                  ...    ...   \n",
       "99995  Car Decal volant faucon Auto Camion capot côté...   0.93   \n",
       "99996  Canne A Peche 138H 2/0 Prov Bend Baitholder Sn...  42.99   \n",
       "99997                      Jan Kjrstad   Du monde entier  23.20   \n",
       "99998  Oreillette sans fil bluetooth v4.1 ultra légèr...  26.99   \n",
       "99999  Pattern : slef design   Tissu: polyester   Len...  58.99   \n",
       "\n",
       "                                            title_tokens  \n",
       "0            [beige, marron, portefeuille, femme, iqzco]  \n",
       "1      [hama, étui, eva, pour, disque, dur, externe, ...  \n",
       "2      [bandeau, cheveux, femme, vintage, bandeau, im...  \n",
       "3      [3pcs, plumes, duvet, polyester, vers, bas, co...  \n",
       "4                     [débardeur, rouge, décoré, fleurs]  \n",
       "...                                                  ...  \n",
       "99995  [applique, mural, luminaire, contemporain, 6w,...  \n",
       "99996                      [harceleurs, l'école, bureau]  \n",
       "99997  [jarretière, optique, duplex, 2.0, mm, multi, ...  \n",
       "99998  [maison, poupée, bois, diy, modèle, miniature,...  \n",
       "99999  [housse, etui, coque, alcatel, one, touch, ido...  \n",
       "\n",
       "[100000 rows x 5 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error(test[\"category\"],prediction[\"category\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ce qu'il reste à faire\n",
    "- utiliser la base de donnée entière\n",
    "- utiliser tous les champs\n",
    "- paralléliser encore plus, probablement avec GPU\n",
    "- tester des variations du modèle pour prendre le meilleur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
